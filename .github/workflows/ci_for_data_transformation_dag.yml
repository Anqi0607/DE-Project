name: CI for data transformation dag

on:
  pull_request:
    branches:
      - main
    paths:
      - 'airflow/**'
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/.google/credentials/dev-sa-key.json

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Write GCP service account key
        run: |
          mkdir -p airflow/.google/credentials
          echo "${{ secrets.GCP_SA_KEY }}" > airflow/.google/credentials/dev-sa-key.json

      # Cache Docker layers to speed up the build process
      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache  # Cache location for docker layers
          key: ${{ runner.os }}-docker-${{ github.sha }}  # Cache key
          restore-keys: |
            ${{ runner.os }}-docker-  # Fallback cache key if specific key not found

      - name: Install Docker Compose
        run: |
          sudo curl -L "https://github.com/docker/compose/releases/download/$(curl -s https://api.github.com/repos/docker/compose/releases/latest | jq -r .tag_name)/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
          sudo chmod +x /usr/local/bin/docker-compose
          docker-compose --version  # Verify installation

      - name: Build Docker image
        run: |
          docker build --cache-from=ci-test-for-load-image -t ci-test-for-load-image -f docker/Dockerfile .

      - name: Run Docker compose
        run: |
          docker-compose -f docker/docker-compose.yml up -d

      - name: Run unit tests in Airflow container
        run: |
          docker exec docker-airflow-worker-1 sh -c "export PYTHONPATH='/opt/airflow/scripts:$PYTHONPATH' \
          && pytest /opt/airflow/tests/unit/test_validate_raw_data.py \
          /opt/airflow/tests/unit/test_transform_raw_data.py \
          /opt/airflow/tests/unit/test_check_bronze_data_quality.py"
      
      - name: Run integration tests in Airflow container
        run: |
          docker exec docker-airflow-worker-1 bash -lc "\
            # 在容器里建一个 /tmp/airflow/logs/scheduler 可写目录 \
            mkdir -p /tmp/airflow/logs/scheduler && \
            # 指定 Airflow 日志目录到 /tmp/airflow/logs \
            export AIRFLOW__LOGGING__BASE_LOG_FOLDER=/tmp/airflow/logs && \
            export AIRFLOW__LOGGING__PROCESSOR_LOG_FOLDER=/tmp/airflow/logs/scheduler && \
            # 设置模块搜索路径并切目录 \
            export PYTHONPATH=/opt/airflow && cd /opt/airflow && \
            # 跑测试 \
            pytest tests/integration/test_data_transformation_dag.py -q \
          "
      
      - name: Install Dbt
        run: |
          pip install dbt

      - name: Install Dbt dependencies
        run: |
          dbt deps
      
      - name: Generate profiles.yml

      - name: Stop Docker containers
        run: |
          docker-compose -f docker/docker-compose.yml down